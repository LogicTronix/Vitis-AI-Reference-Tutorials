{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "qwgTtAyp3C7K"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO v3 model architecture"
      ],
      "metadata": {
        "id": "yjwnv6MYoEgc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zs0uKuG55xC_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\"\"\"\n",
        "Architecture config:\n",
        "- Tuple --> (filters, kernel_size, stride)\n",
        "- List --> ['B', num_repeats] where 'B' is residual block\n",
        "- 'S' --> scale prediction block. Also for computing yolo loss\n",
        "- 'U' --> upsampling the feature map and concatenating with a previous layer\n",
        "\"\"\"\n",
        "config = [\n",
        "    (32, 3, 1),\n",
        "    (64, 3, 2),\n",
        "    [\"B\", 1],\n",
        "    (128, 3, 2),\n",
        "    [\"B\", 2],\n",
        "    (256, 3, 2),\n",
        "    [\"B\", 8],\n",
        "    (512, 3, 2),\n",
        "    [\"B\", 8],\n",
        "    (1024, 3, 2),\n",
        "    [\"B\", 4],  # To this point is Darknet-53\n",
        "\n",
        "    (512, 1, 1),\n",
        "    (1024, 3, 1),\n",
        "    \"S\",\n",
        "    (256, 1, 1),\n",
        "    \"U\",\n",
        "    (256, 1, 1),\n",
        "    (512, 3, 1),\n",
        "    \"S\",\n",
        "    (128, 1, 1),\n",
        "    \"U\",\n",
        "    (128, 1, 1),\n",
        "    (256, 3, 1),\n",
        "    \"S\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
        "    super(CNNBlock, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs) # If batchnorm layer(bn_act) is true, then bias is False\n",
        "    self.bn = nn.BatchNorm2d(out_channels)\n",
        "    self.leaky = nn.LeakyReLU(0.1015625)\n",
        "    self.use_bn_act = bn_act\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.use_bn_act:\n",
        "      return self.leaky(self.bn(self.conv(x)))\n",
        "    else:\n",
        "      return self.conv(x)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, channels, use_residual=True, num_repeats=1):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.layers = nn.ModuleList() # Like regular python list, but is container for pytorch nn modules\n",
        "\n",
        "    for repeat in range(num_repeats):\n",
        "      self.layers += [\n",
        "          nn.Sequential(\n",
        "            CNNBlock(channels, channels//2, kernel_size=1),\n",
        "            CNNBlock(channels//2, channels, kernel_size=3, padding=1)\n",
        "          )\n",
        "      ]\n",
        "\n",
        "    self.use_residual = use_residual\n",
        "    self.num_repeats = num_repeats\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      if self.use_residual:\n",
        "        forwarded_x = x\n",
        "        x = layer(x)\n",
        "        x += forwarded_x\n",
        "      else:\n",
        "        x = layer(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "Q_zaXn4YQCl7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScalePrediction(nn.Module):\n",
        "  def __init__(self, in_channels, num_classes):\n",
        "    super(ScalePrediction, self).__init__()\n",
        "    self.pred = nn.Sequential(\n",
        "        CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
        "        CNNBlock(2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1), # (num_classes + 5) * 3 --> (20+5) for each anchor box which in total is 3\n",
        "    )\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, _, grid_h, grid_w = x.shape\n",
        "    # Pass through the prediction function\n",
        "    pred_result = self.pred(x)\n",
        "    # Combine reshape and permute operations using view\n",
        "    fused_result = pred_result.view(batch_size, 3, grid_h, grid_w, self.num_classes + 5) # [batch_size, anchor_boxes, grid_h, grid_w, prediction(25)]\n",
        "\n",
        "    return fused_result"
      ],
      "metadata": {
        "id": "IUrOUQNeXDcG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLOv3(nn.Module):\n",
        "  def __init__(self, in_channels=3, num_classes=20):\n",
        "    super(YOLOv3, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.in_channels = in_channels\n",
        "    self.layers = self._create_conv_layers()\n",
        "\n",
        "  def forward(self, x):\n",
        "    outputs = []\n",
        "    route_connections = []\n",
        "\n",
        "    for layer in self.layers:\n",
        "      if isinstance(layer, ScalePrediction):\n",
        "        outputs.append(layer(x))\n",
        "        continue\n",
        "\n",
        "      x = layer(x)\n",
        "\n",
        "      if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
        "        route_connections.append(x)\n",
        "\n",
        "      elif isinstance(layer, nn.Upsample):\n",
        "        x = torch.cat([x, route_connections[-1]], dim=1)\n",
        "        route_connections.pop()\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "  def _create_conv_layers(self):\n",
        "    layers = nn.ModuleList()\n",
        "    in_channels = self.in_channels\n",
        "\n",
        "    for module in config:\n",
        "      if isinstance(module, tuple):\n",
        "        out_channels, kernel_size, stride = module\n",
        "        layers.append(CNNBlock(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=1 if kernel_size == 3 else 0\n",
        "        ))\n",
        "        in_channels = out_channels\n",
        "\n",
        "      elif isinstance(module, list):\n",
        "        num_repeats = module[1]\n",
        "        layers.append(ResidualBlock(in_channels, num_repeats=num_repeats))\n",
        "\n",
        "      elif isinstance(module, str):\n",
        "        if module == \"S\":\n",
        "          layers += [\n",
        "              ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
        "              CNNBlock(in_channels, in_channels//2, kernel_size=1),\n",
        "              ScalePrediction(in_channels//2, num_classes = self.num_classes)\n",
        "          ]\n",
        "          in_channels = in_channels // 2\n",
        "\n",
        "        elif module == \"U\":\n",
        "          layers.append(nn.Upsample(scale_factor=2))\n",
        "          in_channels = in_channels * 3\n",
        "\n",
        "    return layers\n"
      ],
      "metadata": {
        "id": "iwYyoG0AQXtj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 20\n",
        "IMAGE_SIZE = 416\n",
        "\n",
        "model = YOLOv3(num_classes = num_classes)\n",
        "x = torch.randn((2,3, IMAGE_SIZE, IMAGE_SIZE))\n",
        "len(model(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S2pyv5SmViF",
        "outputId": "b0590408-ebb4-4adc-cf2d-8e2307380526"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(x)[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc_-au2vmuSG",
        "outputId": "4402c7e8-6650-43f7-9939-db70290c4bc6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 13, 13, 25])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(x)[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfXEjuaGn0Ey",
        "outputId": "9878c17a-a49b-4adf-bf4b-67a3b4e327fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 26, 26, 25])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(x)[2].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efmVBBZ2n3Ea",
        "outputId": "73bf7082-3cc9-4d05-d3d5-9bb0657346f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 52, 52, 25])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NkUzpq0eUHc",
        "outputId": "52243867-c1e1-485e-ebc3-feddb10ccbfa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YOLOv3(\n",
              "  (layers): ModuleList(\n",
              "    (0): CNNBlock(\n",
              "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (1): CNNBlock(\n",
              "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (2): ResidualBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): CNNBlock(\n",
              "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "          (1): CNNBlock(\n",
              "            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): CNNBlock(\n",
              "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (4): ResidualBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): CNNBlock(\n",
              "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "          (1): CNNBlock(\n",
              "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): CNNBlock(\n",
              "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (6): ResidualBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0-7): 8 x Sequential(\n",
              "          (0): CNNBlock(\n",
              "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "          (1): CNNBlock(\n",
              "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): CNNBlock(\n",
              "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (8): ResidualBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0-7): 8 x Sequential(\n",
              "          (0): CNNBlock(\n",
              "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "          (1): CNNBlock(\n",
              "            (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): CNNBlock(\n",
              "      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (10): ResidualBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0-3): 4 x Sequential(\n",
              "          (0): CNNBlock(\n",
              "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "          (1): CNNBlock(\n",
              "            (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): CNNBlock(\n",
              "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (12): CNNBlock(\n",
              "      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (13): ResidualBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): CNNBlock(\n",
              "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "          (1): CNNBlock(\n",
              "            (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (14): CNNBlock(\n",
              "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (15): ScalePrediction(\n",
              "      (pred): Sequential(\n",
              "        (0): CNNBlock(\n",
              "          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "        )\n",
              "        (1): CNNBlock(\n",
              "          (conv): Conv2d(1024, 75, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (bn): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (16): CNNBlock(\n",
              "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (17): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    (18): CNNBlock(\n",
              "      (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (19): CNNBlock(\n",
              "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (20): ResidualBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): CNNBlock(\n",
              "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "          (1): CNNBlock(\n",
              "            (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (21): CNNBlock(\n",
              "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (22): ScalePrediction(\n",
              "      (pred): Sequential(\n",
              "        (0): CNNBlock(\n",
              "          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "        )\n",
              "        (1): CNNBlock(\n",
              "          (conv): Conv2d(512, 75, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (bn): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (23): CNNBlock(\n",
              "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (24): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    (25): CNNBlock(\n",
              "      (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (26): CNNBlock(\n",
              "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (27): ResidualBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): CNNBlock(\n",
              "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "          (1): CNNBlock(\n",
              "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (28): CNNBlock(\n",
              "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "    )\n",
              "    (29): ScalePrediction(\n",
              "      (pred): Sequential(\n",
              "        (0): CNNBlock(\n",
              "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "        )\n",
              "        (1): CNNBlock(\n",
              "          (conv): Conv2d(256, 75, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (bn): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (leaky): LeakyReLU(negative_slope=0.1015625)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# Display the model summary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "summary(model, input_size=(3, 416, 416))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RdxCldQeZTJ",
        "outputId": "92ab8faf-199a-403f-c5dc-fd2e1e720359"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 416, 416]             864\n",
            "       BatchNorm2d-2         [-1, 32, 416, 416]              64\n",
            "         LeakyReLU-3         [-1, 32, 416, 416]               0\n",
            "          CNNBlock-4         [-1, 32, 416, 416]               0\n",
            "            Conv2d-5         [-1, 64, 208, 208]          18,432\n",
            "       BatchNorm2d-6         [-1, 64, 208, 208]             128\n",
            "         LeakyReLU-7         [-1, 64, 208, 208]               0\n",
            "          CNNBlock-8         [-1, 64, 208, 208]               0\n",
            "            Conv2d-9         [-1, 32, 208, 208]           2,048\n",
            "      BatchNorm2d-10         [-1, 32, 208, 208]              64\n",
            "        LeakyReLU-11         [-1, 32, 208, 208]               0\n",
            "         CNNBlock-12         [-1, 32, 208, 208]               0\n",
            "           Conv2d-13         [-1, 64, 208, 208]          18,432\n",
            "      BatchNorm2d-14         [-1, 64, 208, 208]             128\n",
            "        LeakyReLU-15         [-1, 64, 208, 208]               0\n",
            "         CNNBlock-16         [-1, 64, 208, 208]               0\n",
            "    ResidualBlock-17         [-1, 64, 208, 208]               0\n",
            "           Conv2d-18        [-1, 128, 104, 104]          73,728\n",
            "      BatchNorm2d-19        [-1, 128, 104, 104]             256\n",
            "        LeakyReLU-20        [-1, 128, 104, 104]               0\n",
            "         CNNBlock-21        [-1, 128, 104, 104]               0\n",
            "           Conv2d-22         [-1, 64, 104, 104]           8,192\n",
            "      BatchNorm2d-23         [-1, 64, 104, 104]             128\n",
            "        LeakyReLU-24         [-1, 64, 104, 104]               0\n",
            "         CNNBlock-25         [-1, 64, 104, 104]               0\n",
            "           Conv2d-26        [-1, 128, 104, 104]          73,728\n",
            "      BatchNorm2d-27        [-1, 128, 104, 104]             256\n",
            "        LeakyReLU-28        [-1, 128, 104, 104]               0\n",
            "         CNNBlock-29        [-1, 128, 104, 104]               0\n",
            "           Conv2d-30         [-1, 64, 104, 104]           8,192\n",
            "      BatchNorm2d-31         [-1, 64, 104, 104]             128\n",
            "        LeakyReLU-32         [-1, 64, 104, 104]               0\n",
            "         CNNBlock-33         [-1, 64, 104, 104]               0\n",
            "           Conv2d-34        [-1, 128, 104, 104]          73,728\n",
            "      BatchNorm2d-35        [-1, 128, 104, 104]             256\n",
            "        LeakyReLU-36        [-1, 128, 104, 104]               0\n",
            "         CNNBlock-37        [-1, 128, 104, 104]               0\n",
            "    ResidualBlock-38        [-1, 128, 104, 104]               0\n",
            "           Conv2d-39          [-1, 256, 52, 52]         294,912\n",
            "      BatchNorm2d-40          [-1, 256, 52, 52]             512\n",
            "        LeakyReLU-41          [-1, 256, 52, 52]               0\n",
            "         CNNBlock-42          [-1, 256, 52, 52]               0\n",
            "           Conv2d-43          [-1, 128, 52, 52]          32,768\n",
            "      BatchNorm2d-44          [-1, 128, 52, 52]             256\n",
            "        LeakyReLU-45          [-1, 128, 52, 52]               0\n",
            "         CNNBlock-46          [-1, 128, 52, 52]               0\n",
            "           Conv2d-47          [-1, 256, 52, 52]         294,912\n",
            "      BatchNorm2d-48          [-1, 256, 52, 52]             512\n",
            "        LeakyReLU-49          [-1, 256, 52, 52]               0\n",
            "         CNNBlock-50          [-1, 256, 52, 52]               0\n",
            "           Conv2d-51          [-1, 128, 52, 52]          32,768\n",
            "      BatchNorm2d-52          [-1, 128, 52, 52]             256\n",
            "        LeakyReLU-53          [-1, 128, 52, 52]               0\n",
            "         CNNBlock-54          [-1, 128, 52, 52]               0\n",
            "           Conv2d-55          [-1, 256, 52, 52]         294,912\n",
            "      BatchNorm2d-56          [-1, 256, 52, 52]             512\n",
            "        LeakyReLU-57          [-1, 256, 52, 52]               0\n",
            "         CNNBlock-58          [-1, 256, 52, 52]               0\n",
            "           Conv2d-59          [-1, 128, 52, 52]          32,768\n",
            "      BatchNorm2d-60          [-1, 128, 52, 52]             256\n",
            "        LeakyReLU-61          [-1, 128, 52, 52]               0\n",
            "         CNNBlock-62          [-1, 128, 52, 52]               0\n",
            "           Conv2d-63          [-1, 256, 52, 52]         294,912\n",
            "      BatchNorm2d-64          [-1, 256, 52, 52]             512\n",
            "        LeakyReLU-65          [-1, 256, 52, 52]               0\n",
            "         CNNBlock-66          [-1, 256, 52, 52]               0\n",
            "           Conv2d-67          [-1, 128, 52, 52]          32,768\n",
            "      BatchNorm2d-68          [-1, 128, 52, 52]             256\n",
            "        LeakyReLU-69          [-1, 128, 52, 52]               0\n",
            "         CNNBlock-70          [-1, 128, 52, 52]               0\n",
            "           Conv2d-71          [-1, 256, 52, 52]         294,912\n",
            "      BatchNorm2d-72          [-1, 256, 52, 52]             512\n",
            "        LeakyReLU-73          [-1, 256, 52, 52]               0\n",
            "         CNNBlock-74          [-1, 256, 52, 52]               0\n",
            "           Conv2d-75          [-1, 128, 52, 52]          32,768\n",
            "      BatchNorm2d-76          [-1, 128, 52, 52]             256\n",
            "        LeakyReLU-77          [-1, 128, 52, 52]               0\n",
            "         CNNBlock-78          [-1, 128, 52, 52]               0\n",
            "           Conv2d-79          [-1, 256, 52, 52]         294,912\n",
            "      BatchNorm2d-80          [-1, 256, 52, 52]             512\n",
            "        LeakyReLU-81          [-1, 256, 52, 52]               0\n",
            "         CNNBlock-82          [-1, 256, 52, 52]               0\n",
            "           Conv2d-83          [-1, 128, 52, 52]          32,768\n",
            "      BatchNorm2d-84          [-1, 128, 52, 52]             256\n",
            "        LeakyReLU-85          [-1, 128, 52, 52]               0\n",
            "         CNNBlock-86          [-1, 128, 52, 52]               0\n",
            "           Conv2d-87          [-1, 256, 52, 52]         294,912\n",
            "      BatchNorm2d-88          [-1, 256, 52, 52]             512\n",
            "        LeakyReLU-89          [-1, 256, 52, 52]               0\n",
            "         CNNBlock-90          [-1, 256, 52, 52]               0\n",
            "           Conv2d-91          [-1, 128, 52, 52]          32,768\n",
            "      BatchNorm2d-92          [-1, 128, 52, 52]             256\n",
            "        LeakyReLU-93          [-1, 128, 52, 52]               0\n",
            "         CNNBlock-94          [-1, 128, 52, 52]               0\n",
            "           Conv2d-95          [-1, 256, 52, 52]         294,912\n",
            "      BatchNorm2d-96          [-1, 256, 52, 52]             512\n",
            "        LeakyReLU-97          [-1, 256, 52, 52]               0\n",
            "         CNNBlock-98          [-1, 256, 52, 52]               0\n",
            "           Conv2d-99          [-1, 128, 52, 52]          32,768\n",
            "     BatchNorm2d-100          [-1, 128, 52, 52]             256\n",
            "       LeakyReLU-101          [-1, 128, 52, 52]               0\n",
            "        CNNBlock-102          [-1, 128, 52, 52]               0\n",
            "          Conv2d-103          [-1, 256, 52, 52]         294,912\n",
            "     BatchNorm2d-104          [-1, 256, 52, 52]             512\n",
            "       LeakyReLU-105          [-1, 256, 52, 52]               0\n",
            "        CNNBlock-106          [-1, 256, 52, 52]               0\n",
            "   ResidualBlock-107          [-1, 256, 52, 52]               0\n",
            "          Conv2d-108          [-1, 512, 26, 26]       1,179,648\n",
            "     BatchNorm2d-109          [-1, 512, 26, 26]           1,024\n",
            "       LeakyReLU-110          [-1, 512, 26, 26]               0\n",
            "        CNNBlock-111          [-1, 512, 26, 26]               0\n",
            "          Conv2d-112          [-1, 256, 26, 26]         131,072\n",
            "     BatchNorm2d-113          [-1, 256, 26, 26]             512\n",
            "       LeakyReLU-114          [-1, 256, 26, 26]               0\n",
            "        CNNBlock-115          [-1, 256, 26, 26]               0\n",
            "          Conv2d-116          [-1, 512, 26, 26]       1,179,648\n",
            "     BatchNorm2d-117          [-1, 512, 26, 26]           1,024\n",
            "       LeakyReLU-118          [-1, 512, 26, 26]               0\n",
            "        CNNBlock-119          [-1, 512, 26, 26]               0\n",
            "          Conv2d-120          [-1, 256, 26, 26]         131,072\n",
            "     BatchNorm2d-121          [-1, 256, 26, 26]             512\n",
            "       LeakyReLU-122          [-1, 256, 26, 26]               0\n",
            "        CNNBlock-123          [-1, 256, 26, 26]               0\n",
            "          Conv2d-124          [-1, 512, 26, 26]       1,179,648\n",
            "     BatchNorm2d-125          [-1, 512, 26, 26]           1,024\n",
            "       LeakyReLU-126          [-1, 512, 26, 26]               0\n",
            "        CNNBlock-127          [-1, 512, 26, 26]               0\n",
            "          Conv2d-128          [-1, 256, 26, 26]         131,072\n",
            "     BatchNorm2d-129          [-1, 256, 26, 26]             512\n",
            "       LeakyReLU-130          [-1, 256, 26, 26]               0\n",
            "        CNNBlock-131          [-1, 256, 26, 26]               0\n",
            "          Conv2d-132          [-1, 512, 26, 26]       1,179,648\n",
            "     BatchNorm2d-133          [-1, 512, 26, 26]           1,024\n",
            "       LeakyReLU-134          [-1, 512, 26, 26]               0\n",
            "        CNNBlock-135          [-1, 512, 26, 26]               0\n",
            "          Conv2d-136          [-1, 256, 26, 26]         131,072\n",
            "     BatchNorm2d-137          [-1, 256, 26, 26]             512\n",
            "       LeakyReLU-138          [-1, 256, 26, 26]               0\n",
            "        CNNBlock-139          [-1, 256, 26, 26]               0\n",
            "          Conv2d-140          [-1, 512, 26, 26]       1,179,648\n",
            "     BatchNorm2d-141          [-1, 512, 26, 26]           1,024\n",
            "       LeakyReLU-142          [-1, 512, 26, 26]               0\n",
            "        CNNBlock-143          [-1, 512, 26, 26]               0\n",
            "          Conv2d-144          [-1, 256, 26, 26]         131,072\n",
            "     BatchNorm2d-145          [-1, 256, 26, 26]             512\n",
            "       LeakyReLU-146          [-1, 256, 26, 26]               0\n",
            "        CNNBlock-147          [-1, 256, 26, 26]               0\n",
            "          Conv2d-148          [-1, 512, 26, 26]       1,179,648\n",
            "     BatchNorm2d-149          [-1, 512, 26, 26]           1,024\n",
            "       LeakyReLU-150          [-1, 512, 26, 26]               0\n",
            "        CNNBlock-151          [-1, 512, 26, 26]               0\n",
            "          Conv2d-152          [-1, 256, 26, 26]         131,072\n",
            "     BatchNorm2d-153          [-1, 256, 26, 26]             512\n",
            "       LeakyReLU-154          [-1, 256, 26, 26]               0\n",
            "        CNNBlock-155          [-1, 256, 26, 26]               0\n",
            "          Conv2d-156          [-1, 512, 26, 26]       1,179,648\n",
            "     BatchNorm2d-157          [-1, 512, 26, 26]           1,024\n",
            "       LeakyReLU-158          [-1, 512, 26, 26]               0\n",
            "        CNNBlock-159          [-1, 512, 26, 26]               0\n",
            "          Conv2d-160          [-1, 256, 26, 26]         131,072\n",
            "     BatchNorm2d-161          [-1, 256, 26, 26]             512\n",
            "       LeakyReLU-162          [-1, 256, 26, 26]               0\n",
            "        CNNBlock-163          [-1, 256, 26, 26]               0\n",
            "          Conv2d-164          [-1, 512, 26, 26]       1,179,648\n",
            "     BatchNorm2d-165          [-1, 512, 26, 26]           1,024\n",
            "       LeakyReLU-166          [-1, 512, 26, 26]               0\n",
            "        CNNBlock-167          [-1, 512, 26, 26]               0\n",
            "          Conv2d-168          [-1, 256, 26, 26]         131,072\n",
            "     BatchNorm2d-169          [-1, 256, 26, 26]             512\n",
            "       LeakyReLU-170          [-1, 256, 26, 26]               0\n",
            "        CNNBlock-171          [-1, 256, 26, 26]               0\n",
            "          Conv2d-172          [-1, 512, 26, 26]       1,179,648\n",
            "     BatchNorm2d-173          [-1, 512, 26, 26]           1,024\n",
            "       LeakyReLU-174          [-1, 512, 26, 26]               0\n",
            "        CNNBlock-175          [-1, 512, 26, 26]               0\n",
            "   ResidualBlock-176          [-1, 512, 26, 26]               0\n",
            "          Conv2d-177         [-1, 1024, 13, 13]       4,718,592\n",
            "     BatchNorm2d-178         [-1, 1024, 13, 13]           2,048\n",
            "       LeakyReLU-179         [-1, 1024, 13, 13]               0\n",
            "        CNNBlock-180         [-1, 1024, 13, 13]               0\n",
            "          Conv2d-181          [-1, 512, 13, 13]         524,288\n",
            "     BatchNorm2d-182          [-1, 512, 13, 13]           1,024\n",
            "       LeakyReLU-183          [-1, 512, 13, 13]               0\n",
            "        CNNBlock-184          [-1, 512, 13, 13]               0\n",
            "          Conv2d-185         [-1, 1024, 13, 13]       4,718,592\n",
            "     BatchNorm2d-186         [-1, 1024, 13, 13]           2,048\n",
            "       LeakyReLU-187         [-1, 1024, 13, 13]               0\n",
            "        CNNBlock-188         [-1, 1024, 13, 13]               0\n",
            "          Conv2d-189          [-1, 512, 13, 13]         524,288\n",
            "     BatchNorm2d-190          [-1, 512, 13, 13]           1,024\n",
            "       LeakyReLU-191          [-1, 512, 13, 13]               0\n",
            "        CNNBlock-192          [-1, 512, 13, 13]               0\n",
            "          Conv2d-193         [-1, 1024, 13, 13]       4,718,592\n",
            "     BatchNorm2d-194         [-1, 1024, 13, 13]           2,048\n",
            "       LeakyReLU-195         [-1, 1024, 13, 13]               0\n",
            "        CNNBlock-196         [-1, 1024, 13, 13]               0\n",
            "          Conv2d-197          [-1, 512, 13, 13]         524,288\n",
            "     BatchNorm2d-198          [-1, 512, 13, 13]           1,024\n",
            "       LeakyReLU-199          [-1, 512, 13, 13]               0\n",
            "        CNNBlock-200          [-1, 512, 13, 13]               0\n",
            "          Conv2d-201         [-1, 1024, 13, 13]       4,718,592\n",
            "     BatchNorm2d-202         [-1, 1024, 13, 13]           2,048\n",
            "       LeakyReLU-203         [-1, 1024, 13, 13]               0\n",
            "        CNNBlock-204         [-1, 1024, 13, 13]               0\n",
            "          Conv2d-205          [-1, 512, 13, 13]         524,288\n",
            "     BatchNorm2d-206          [-1, 512, 13, 13]           1,024\n",
            "       LeakyReLU-207          [-1, 512, 13, 13]               0\n",
            "        CNNBlock-208          [-1, 512, 13, 13]               0\n",
            "          Conv2d-209         [-1, 1024, 13, 13]       4,718,592\n",
            "     BatchNorm2d-210         [-1, 1024, 13, 13]           2,048\n",
            "       LeakyReLU-211         [-1, 1024, 13, 13]               0\n",
            "        CNNBlock-212         [-1, 1024, 13, 13]               0\n",
            "   ResidualBlock-213         [-1, 1024, 13, 13]               0\n",
            "          Conv2d-214          [-1, 512, 13, 13]         524,288\n",
            "     BatchNorm2d-215          [-1, 512, 13, 13]           1,024\n",
            "       LeakyReLU-216          [-1, 512, 13, 13]               0\n",
            "        CNNBlock-217          [-1, 512, 13, 13]               0\n",
            "          Conv2d-218         [-1, 1024, 13, 13]       4,718,592\n",
            "     BatchNorm2d-219         [-1, 1024, 13, 13]           2,048\n",
            "       LeakyReLU-220         [-1, 1024, 13, 13]               0\n",
            "        CNNBlock-221         [-1, 1024, 13, 13]               0\n",
            "          Conv2d-222          [-1, 512, 13, 13]         524,288\n",
            "     BatchNorm2d-223          [-1, 512, 13, 13]           1,024\n",
            "       LeakyReLU-224          [-1, 512, 13, 13]               0\n",
            "        CNNBlock-225          [-1, 512, 13, 13]               0\n",
            "          Conv2d-226         [-1, 1024, 13, 13]       4,718,592\n",
            "     BatchNorm2d-227         [-1, 1024, 13, 13]           2,048\n",
            "       LeakyReLU-228         [-1, 1024, 13, 13]               0\n",
            "        CNNBlock-229         [-1, 1024, 13, 13]               0\n",
            "   ResidualBlock-230         [-1, 1024, 13, 13]               0\n",
            "          Conv2d-231          [-1, 512, 13, 13]         524,288\n",
            "     BatchNorm2d-232          [-1, 512, 13, 13]           1,024\n",
            "       LeakyReLU-233          [-1, 512, 13, 13]               0\n",
            "        CNNBlock-234          [-1, 512, 13, 13]               0\n",
            "          Conv2d-235         [-1, 1024, 13, 13]       4,718,592\n",
            "     BatchNorm2d-236         [-1, 1024, 13, 13]           2,048\n",
            "       LeakyReLU-237         [-1, 1024, 13, 13]               0\n",
            "        CNNBlock-238         [-1, 1024, 13, 13]               0\n",
            "          Conv2d-239           [-1, 75, 13, 13]          76,875\n",
            "        CNNBlock-240           [-1, 75, 13, 13]               0\n",
            " ScalePrediction-241        [-1, 3, 13, 13, 25]               0\n",
            "          Conv2d-242          [-1, 256, 13, 13]         131,072\n",
            "     BatchNorm2d-243          [-1, 256, 13, 13]             512\n",
            "       LeakyReLU-244          [-1, 256, 13, 13]               0\n",
            "        CNNBlock-245          [-1, 256, 13, 13]               0\n",
            "        Upsample-246          [-1, 256, 26, 26]               0\n",
            "          Conv2d-247          [-1, 256, 26, 26]         196,608\n",
            "     BatchNorm2d-248          [-1, 256, 26, 26]             512\n",
            "       LeakyReLU-249          [-1, 256, 26, 26]               0\n",
            "        CNNBlock-250          [-1, 256, 26, 26]               0\n",
            "          Conv2d-251          [-1, 512, 26, 26]       1,179,648\n",
            "     BatchNorm2d-252          [-1, 512, 26, 26]           1,024\n",
            "       LeakyReLU-253          [-1, 512, 26, 26]               0\n",
            "        CNNBlock-254          [-1, 512, 26, 26]               0\n",
            "          Conv2d-255          [-1, 256, 26, 26]         131,072\n",
            "     BatchNorm2d-256          [-1, 256, 26, 26]             512\n",
            "       LeakyReLU-257          [-1, 256, 26, 26]               0\n",
            "        CNNBlock-258          [-1, 256, 26, 26]               0\n",
            "          Conv2d-259          [-1, 512, 26, 26]       1,179,648\n",
            "     BatchNorm2d-260          [-1, 512, 26, 26]           1,024\n",
            "       LeakyReLU-261          [-1, 512, 26, 26]               0\n",
            "        CNNBlock-262          [-1, 512, 26, 26]               0\n",
            "   ResidualBlock-263          [-1, 512, 26, 26]               0\n",
            "          Conv2d-264          [-1, 256, 26, 26]         131,072\n",
            "     BatchNorm2d-265          [-1, 256, 26, 26]             512\n",
            "       LeakyReLU-266          [-1, 256, 26, 26]               0\n",
            "        CNNBlock-267          [-1, 256, 26, 26]               0\n",
            "          Conv2d-268          [-1, 512, 26, 26]       1,179,648\n",
            "     BatchNorm2d-269          [-1, 512, 26, 26]           1,024\n",
            "       LeakyReLU-270          [-1, 512, 26, 26]               0\n",
            "        CNNBlock-271          [-1, 512, 26, 26]               0\n",
            "          Conv2d-272           [-1, 75, 26, 26]          38,475\n",
            "        CNNBlock-273           [-1, 75, 26, 26]               0\n",
            " ScalePrediction-274        [-1, 3, 26, 26, 25]               0\n",
            "          Conv2d-275          [-1, 128, 26, 26]          32,768\n",
            "     BatchNorm2d-276          [-1, 128, 26, 26]             256\n",
            "       LeakyReLU-277          [-1, 128, 26, 26]               0\n",
            "        CNNBlock-278          [-1, 128, 26, 26]               0\n",
            "        Upsample-279          [-1, 128, 52, 52]               0\n",
            "          Conv2d-280          [-1, 128, 52, 52]          49,152\n",
            "     BatchNorm2d-281          [-1, 128, 52, 52]             256\n",
            "       LeakyReLU-282          [-1, 128, 52, 52]               0\n",
            "        CNNBlock-283          [-1, 128, 52, 52]               0\n",
            "          Conv2d-284          [-1, 256, 52, 52]         294,912\n",
            "     BatchNorm2d-285          [-1, 256, 52, 52]             512\n",
            "       LeakyReLU-286          [-1, 256, 52, 52]               0\n",
            "        CNNBlock-287          [-1, 256, 52, 52]               0\n",
            "          Conv2d-288          [-1, 128, 52, 52]          32,768\n",
            "     BatchNorm2d-289          [-1, 128, 52, 52]             256\n",
            "       LeakyReLU-290          [-1, 128, 52, 52]               0\n",
            "        CNNBlock-291          [-1, 128, 52, 52]               0\n",
            "          Conv2d-292          [-1, 256, 52, 52]         294,912\n",
            "     BatchNorm2d-293          [-1, 256, 52, 52]             512\n",
            "       LeakyReLU-294          [-1, 256, 52, 52]               0\n",
            "        CNNBlock-295          [-1, 256, 52, 52]               0\n",
            "   ResidualBlock-296          [-1, 256, 52, 52]               0\n",
            "          Conv2d-297          [-1, 128, 52, 52]          32,768\n",
            "     BatchNorm2d-298          [-1, 128, 52, 52]             256\n",
            "       LeakyReLU-299          [-1, 128, 52, 52]               0\n",
            "        CNNBlock-300          [-1, 128, 52, 52]               0\n",
            "          Conv2d-301          [-1, 256, 52, 52]         294,912\n",
            "     BatchNorm2d-302          [-1, 256, 52, 52]             512\n",
            "       LeakyReLU-303          [-1, 256, 52, 52]               0\n",
            "        CNNBlock-304          [-1, 256, 52, 52]               0\n",
            "          Conv2d-305           [-1, 75, 52, 52]          19,275\n",
            "        CNNBlock-306           [-1, 75, 52, 52]               0\n",
            " ScalePrediction-307        [-1, 3, 52, 52, 25]               0\n",
            "================================================================\n",
            "Total params: 61,626,049\n",
            "Trainable params: 61,626,049\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.98\n",
            "Forward/backward pass size (MB): 1228.70\n",
            "Params size (MB): 235.08\n",
            "Estimated Total Size (MB): 1465.77\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config File"
      ],
      "metadata": {
        "id": "dj8JoGrytb6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "NUM_WORKERS = 4\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 416\n",
        "NUM_CLASSES = 20\n",
        "LEARNING_RATE = 1e-5\n",
        "NUM_EPOCHS = 20\n",
        "CONF_THRESHOLD = 0.05\n",
        "MAP_IOU_THRESH = 0.5\n",
        "NMS_IOU_THRESH = 0.45\n",
        "S = [IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8]\n",
        "\n",
        "IMG_DIR = \"/content/drive/MyDrive/500images\"\n",
        "LABEL_DIR = \"/content/drive/MyDrive/500labels\"\n",
        "\n",
        "ANCHORS = [\n",
        "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
        "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
        "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
        "]  # Note these have been rescaled to be between [0, 1]\n",
        "\n",
        "\n",
        "PASCAL_CLASSES = [\n",
        "    \"aeroplane\",\n",
        "    \"bicycle\",\n",
        "    \"bird\",\n",
        "    \"boat\",\n",
        "    \"bottle\",\n",
        "    \"bus\",\n",
        "    \"car\",\n",
        "    \"cat\",\n",
        "    \"chair\",\n",
        "    \"cow\",\n",
        "    \"diningtable\",\n",
        "    \"dog\",\n",
        "    \"horse\",\n",
        "    \"motorbike\",\n",
        "    \"person\",\n",
        "    \"pottedplant\",\n",
        "    \"sheep\",\n",
        "    \"sofa\",\n",
        "    \"train\",\n",
        "    \"tvmonitor\"\n",
        "]"
      ],
      "metadata": {
        "id": "9Ai7PMaUteOY"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IOU width height\n",
        "- Take in hxw of anchor boxe and bounding box to calc. IOU"
      ],
      "metadata": {
        "id": "aGho3C7Zw5jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iou_width_height(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        boxes1 (tensor): width and height of the first bounding boxes\n",
        "        boxes2 (tensor): width and height of the second bounding boxes\n",
        "    Returns:\n",
        "        tensor: Intersection over union of the corresponding boxes\n",
        "    \"\"\"\n",
        "    intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n",
        "        boxes1[..., 1], boxes2[..., 1]\n",
        "    )\n",
        "    union = (\n",
        "        boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n",
        "    )\n",
        "    return intersection / union"
      ],
      "metadata": {
        "id": "4P448jivxbnu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intersection over union"
      ],
      "metadata": {
        "id": "uXiVIrYjxfnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "    \"\"\"\n",
        "    This function calculates intersection over union (iou) given pred boxes\n",
        "    and target boxes.\n",
        "\n",
        "    Parameters:\n",
        "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
        "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
        "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
        "\n",
        "    Returns:\n",
        "        tensor: Intersection over union for all examples\n",
        "    \"\"\"\n",
        "\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "    if box_format == \"corners\":\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
      ],
      "metadata": {
        "id": "RvwUplkYxqrw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-max Supression"
      ],
      "metadata": {
        "id": "IkcXigCkxv9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
        "    \"\"\"\n",
        "     Does Non Max Suppression given bboxes\n",
        "\n",
        "    Parameters:\n",
        "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
        "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
        "        iou_threshold (float): threshold where predicted bboxes is correct\n",
        "        threshold (float): threshold to remove predicted bboxes (independent of IoU)\n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "\n",
        "    Returns:\n",
        "        list: bboxes after performing NMS given a specific IoU threshold\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(bboxes) == list\n",
        "\n",
        "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "    bboxes_after_nms = []\n",
        "\n",
        "    while bboxes:\n",
        "        chosen_box = bboxes.pop(0)\n",
        "\n",
        "        bboxes = [\n",
        "            box\n",
        "            for box in bboxes\n",
        "            if box[0] != chosen_box[0]\n",
        "            or intersection_over_union(\n",
        "                torch.tensor(chosen_box[2:]),\n",
        "                torch.tensor(box[2:]),\n",
        "                box_format=box_format,\n",
        "            )\n",
        "            < iou_threshold\n",
        "        ]\n",
        "\n",
        "        bboxes_after_nms.append(chosen_box)\n",
        "\n",
        "    return bboxes_after_nms"
      ],
      "metadata": {
        "id": "0oIq_NZpxzNi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "6qWub9xCoJQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image, ImageFile"
      ],
      "metadata": {
        "id": "17VUsG5An6ED"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# allows PIL to load images even if they are truncated or incomplete\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "metadata": {
        "id": "Lfov94HNpy4C"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLODataset(Dataset):\n",
        "  def __init__(self, csv_file, img_dir, label_dir, anchors,\n",
        "               image_size=416, S=[13,26,52], C=20, transform=None):\n",
        "    self.annotations = pd.read_csv(csv_file)\n",
        "    self.img_dir = img_dir\n",
        "    self.label_dir = label_dir\n",
        "    self.transform = transform\n",
        "    self.S = S\n",
        "\n",
        "    # Suppose, anchors[0] = [a,b,c], anchors[1] = [d,e,f], anchors[2] = [g,h,i] : Each set of anchors for each scale\n",
        "    # List addition gives shape 3x3\n",
        "    # Anchors per scale suggests that there are three different aspect ratios for each anchor position.\n",
        "    self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2]) # For all 3 scales\n",
        "    self.num_anchors = self.anchors.shape[0]\n",
        "    self.num_anchors_per_scale = self.num_anchors // 3\n",
        "\n",
        "    self.C = C\n",
        "\n",
        "    # If a cell has obj. then one anchor is responsible for outputting it,\n",
        "    # one that's responsible is the one that has highest iou with ground truth box\n",
        "    # but, there might be cases where there are several boxes in the same cell\n",
        "    self.ignore_iou_thresh = 0.5\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
        "    bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist() # np.roll with shift 4 on axis 1: [class, x, y, w, h] --> [x, y, w, h, class]\n",
        "\n",
        "    img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
        "    image = Image.open(img_path)\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S] # 6 because objectness score, bounding box coordinates (x, y, w, h), class label\n",
        "\n",
        "    for box in bboxes:\n",
        "      \"\"\"For each box in bboxes,\n",
        "      we want to assign which anchor should be responsible and\n",
        "      which cell should be responsible for all the three different scales prediction\"\"\"\n",
        "      iou_anchors = iou_width_height(torch.tensor(box[2:4]), self.anchors) # IOU from height and width\n",
        "      anchor_indices = iou_anchors.argsort(descending=True, dim=0) # Sorting sucht that the first is the best anchor\n",
        "\n",
        "      x, y, width, height, class_label = box\n",
        "      has_anchor = [False, False, False] # Make sure there is an anchor for each of three scales for each bounding box\n",
        "\n",
        "      for anchor_idx in anchor_indices:\n",
        "        scale_idx = anchor_idx // self.num_anchors_per_scale # scale_idx is either 0,1,2: 0-->13x13, 1:-->26x26, 2:-->52x52\n",
        "        anchor_on_scale = anchor_idx % self.num_anchors_per_scale # In each scale, choosing the anchor thats either 0,1,2\n",
        "\n",
        "        S = self.S[scale_idx]\n",
        "        i, j = int(S*y), int(S*x) # x=0.5, S=13 --> int(6.5) = 6 | i=y cell, j=x cell\n",
        "        anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
        "\n",
        "        if not anchor_taken and not has_anchor[scale_idx]:\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
        "          x_cell, y_cell = S*x - j, S*y - i # 6.5 - 6 = 0.5 such that they are between [0,1]\n",
        "          width_cell, height_cell = (\n",
        "              width*S, # S=13, width=0.5, 6.5\n",
        "              height*S\n",
        "          )\n",
        "\n",
        "          box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n",
        "\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
        "          has_anchor[scale_idx] = True\n",
        "\n",
        "        # Even if the same grid shares another anchor having iou>ignore_iou_thresh then,\n",
        "        elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 0] = -1 # ignore this prediction\n",
        "\n",
        "    return image, tuple(targets)\n"
      ],
      "metadata": {
        "id": "UiM8IiJeqIV4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "lJRincdJzCyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.Resize((416, 416)), transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "BsljMXYuXndg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loaders(train_csv_path, test_csv_path):\n",
        "\n",
        "    train_dataset = YOLODataset(\n",
        "        train_csv_path,\n",
        "        transform=transform,\n",
        "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "        img_dir=IMG_DIR,\n",
        "        label_dir=LABEL_DIR,\n",
        "        anchors=ANCHORS,\n",
        "    )\n",
        "    test_dataset = YOLODataset(\n",
        "        test_csv_path,\n",
        "        transform=transform,\n",
        "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "        img_dir=IMG_DIR,\n",
        "        label_dir=LABEL_DIR,\n",
        "        anchors=ANCHORS,\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "10SzzqKdzExo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = YOLODataset(\n",
        "        \"/content/drive/MyDrive/500examples.csv\",\n",
        "        transform=transform,\n",
        "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "        img_dir=IMG_DIR,\n",
        "        label_dir=LABEL_DIR,\n",
        "        anchors=ANCHORS,\n",
        "    )"
      ],
      "metadata": {
        "id": "MZ17ifE7VmC5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )"
      ],
      "metadata": {
        "id": "h8xyltWKV2VO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature, labels = train_dataset[0]"
      ],
      "metadata": {
        "id": "x_X0KTZ_WBlq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKONCE3dWNRC",
        "outputId": "e7379d0e-b9a3-45ea-b466-082012662215"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 416, 416])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature = feature.reshape(-1, 3, 416, 416)"
      ],
      "metadata": {
        "id": "fw6AHgD2aqfI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label13, label26, label52 = labels"
      ],
      "metadata": {
        "id": "azZPSZLuWPF7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label13.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu38sBB7WfVt",
        "outputId": "d9bb8c4d-831c-4be6-eb84-fead911fa4c1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 13, 13, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label26.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opTZUerlWg5G",
        "outputId": "0fdc5191-7e1d-4cfe-faa2-3c65fcfec564"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 26, 26, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label52.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGlRxyxmWifR",
        "outputId": "c80a3a5f-40e1-41d8-9563-af4289786254"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 52, 52, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(feature.to(DEVICE))\n",
        "len(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVfnOzWRaXQl",
        "outputId": "cd57d23f-c5be-4362-c67c-4f4adf020000"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "id": "BlYBUIFvh9yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YoloLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(YoloLoss, self).__init__()\n",
        "    self.mse = nn.MSELoss() # For bounding box loss\n",
        "    self.bce = nn.BCEWithLogitsLoss() # For multi-label prediction: Binary cross entropy\n",
        "    self.entropy = nn.CrossEntropyLoss() # For classification\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Constants for significance of obj, or no obj.\n",
        "    self.lambda_class = 1\n",
        "    self.lambda_noobj = 10\n",
        "    self.lambda_obj = 1\n",
        "    self.lambda_box = 10\n",
        "\n",
        "  def forward(self, predictions, target, anchors):\n",
        "    obj = target[..., 0] == 1\n",
        "    noobj = target[..., 0] == 0\n",
        "\n",
        "    # No object Loss\n",
        "    ################\n",
        "    no_object_loss = self.bce(\n",
        "        (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj])\n",
        "    )\n",
        "\n",
        "    # Object Loss\n",
        "    #############\n",
        "    anchors = anchors.reshape(1,3,1,1,2) # Anchors initial shape 3x2 --> 3 anchor boxes each of certain hxw (2)\n",
        "\n",
        "    # box_preds = [..., sigmoid(x), sigmoid(y), [p_w * exp(t_w)], [p_h * exp(t_h)], ...]\n",
        "    box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n",
        "\n",
        "    # iou between predicted box and target box\n",
        "    ious = intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n",
        "\n",
        "    object_loss = self.bce(\n",
        "        (predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]) # target * iou because only intersected part object loss calc\n",
        "    )\n",
        "\n",
        "    # Box Coordinate Loss\n",
        "    #####################\n",
        "    predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3]) # x, y to be between [0,1]\n",
        "    target[..., 3:5] = torch.log(\n",
        "        (1e-6 + target[..., 3:5] / anchors)\n",
        "    ) # Exponential of hxw (taking log because opp. of exp)\n",
        "\n",
        "    box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n",
        "\n",
        "    # Class Loss\n",
        "    ############\n",
        "    class_loss = self.entropy(\n",
        "        (predictions[..., 5:][obj]), (target[..., 5][obj].long())\n",
        "    )\n",
        "\n",
        "    return(\n",
        "        self.lambda_box * box_loss\n",
        "        + self.lambda_obj * object_loss\n",
        "        + self.lambda_noobj * no_object_loss\n",
        "        + self.lambda_class * class_loss\n",
        "    )\n"
      ],
      "metadata": {
        "id": "i5nJTWLRiAY1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Image"
      ],
      "metadata": {
        "id": "M2mlDBxU2pjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image(image, boxes):\n",
        "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
        "    cmap = plt.get_cmap(\"tab20b\")\n",
        "    class_labels = PASCAL_CLASSES\n",
        "    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n",
        "    im = np.array(image)\n",
        "    height, width, _ = im.shape\n",
        "\n",
        "    # Create figure and axes\n",
        "    fig, ax = plt.subplots(1)\n",
        "    # Display the image\n",
        "    ax.imshow(im)\n",
        "\n",
        "    # box[0] is x midpoint, box[2] is width\n",
        "    # box[1] is y midpoint, box[3] is height\n",
        "\n",
        "    # Create a Rectangle patch\n",
        "    for box in boxes:\n",
        "        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n",
        "        class_pred = box[0]\n",
        "        box = box[2:]\n",
        "        upper_left_x = box[0] - box[2] / 2\n",
        "        upper_left_y = box[1] - box[3] / 2\n",
        "        rect = patches.Rectangle(\n",
        "            (upper_left_x * width, upper_left_y * height),\n",
        "            box[2] * width,\n",
        "            box[3] * height,\n",
        "            linewidth=2,\n",
        "            edgecolor=colors[int(class_pred)],\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(\n",
        "            upper_left_x * width,\n",
        "            upper_left_y * height,\n",
        "            s=class_labels[int(class_pred)],\n",
        "            color=\"white\",\n",
        "            verticalalignment=\"top\",\n",
        "            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
        "        )\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Xjl_AldR2rX9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check accuracy"
      ],
      "metadata": {
        "id": "w5ziHY23iMbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_class_accuracy(model, loader, threshold):\n",
        "    model.eval()\n",
        "    tot_class_preds, correct_class = 0, 0\n",
        "    tot_noobj, correct_noobj = 0, 0\n",
        "    tot_obj, correct_obj = 0, 0\n",
        "\n",
        "    for idx, (x, y) in enumerate(tqdm(loader)):\n",
        "        x = x.to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            out = model(x)\n",
        "\n",
        "        for i in range(3):\n",
        "            y[i] = y[i].to(DEVICE)\n",
        "            obj = y[i][..., 0] == 1 # in paper this is Iobj_i\n",
        "            noobj = y[i][..., 0] == 0  # in paper this is Iobj_i\n",
        "\n",
        "            correct_class += torch.sum(\n",
        "                torch.argmax(out[i][..., 5:][obj], dim=-1) == y[i][..., 5][obj]\n",
        "            )\n",
        "            tot_class_preds += torch.sum(obj)\n",
        "\n",
        "            obj_preds = torch.sigmoid(out[i][..., 0]) > threshold\n",
        "            correct_obj += torch.sum(obj_preds[obj] == y[i][..., 0][obj])\n",
        "            tot_obj += torch.sum(obj)\n",
        "            correct_noobj += torch.sum(obj_preds[noobj] == y[i][..., 0][noobj])\n",
        "            tot_noobj += torch.sum(noobj)\n",
        "\n",
        "    print(f\"Class accuracy is: {(correct_class/(tot_class_preds+1e-16))*100:2f}%\")\n",
        "    print(f\"No obj accuracy is: {(correct_noobj/(tot_noobj+1e-16))*100:2f}%\")\n",
        "    print(f\"Obj accuracy is: {(correct_obj/(tot_obj+1e-16))*100:2f}%\")\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "vf_UTEM-goU5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mean Average Precision"
      ],
      "metadata": {
        "id": "Nk6gBsvpgwPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_precision(\n",
        "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=4\n",
        "):\n",
        "    \"\"\"\n",
        "    Video explanation of this function:\n",
        "    https://youtu.be/FppOzcDvaDI\n",
        "\n",
        "    This function calculates mean average precision (mAP)\n",
        "\n",
        "    Parameters:\n",
        "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
        "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
        "        true_boxes (list): Similar as pred_boxes except all the correct ones\n",
        "        iou_threshold (float): threshold where predicted bboxes is correct\n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "        num_classes (int): number of classes\n",
        "\n",
        "    Returns:\n",
        "        float: mAP value across all classes given a specific IoU threshold\n",
        "    \"\"\"\n",
        "\n",
        "    # list storing all AP for respective classes\n",
        "    average_precisions = []\n",
        "\n",
        "    # used for numerical stability later on\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        detections = []\n",
        "        ground_truths = []\n",
        "\n",
        "        # Go through all predictions and targets,\n",
        "        # and only add the ones that belong to the\n",
        "        # current class c\n",
        "        for detection in pred_boxes:\n",
        "            if detection[1] == c:\n",
        "                detections.append(detection)\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c:\n",
        "                ground_truths.append(true_box)\n",
        "\n",
        "        # find the amount of bboxes for each training example\n",
        "        # Counter here finds how many ground truth bboxes we get\n",
        "        # for each training example, so let's say img 0 has 3,\n",
        "        # img 1 has 5 then we will obtain a dictionary with:\n",
        "        # amount_bboxes = {0:3, 1:5}\n",
        "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "\n",
        "        # We then go through each key, val in this dictionary\n",
        "        # and convert to the following (w.r.t same example):\n",
        "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
        "        for key, val in amount_bboxes.items():\n",
        "            amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "        # sort by box probabilities which is index 2\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "        TP = torch.zeros((len(detections)))\n",
        "        FP = torch.zeros((len(detections)))\n",
        "        total_true_bboxes = len(ground_truths)\n",
        "\n",
        "        # If none exists for this class then we can safely skip\n",
        "        if total_true_bboxes == 0:\n",
        "            continue\n",
        "\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            # Only take out the ground_truths that have the same\n",
        "            # training idx as detection\n",
        "            ground_truth_img = [\n",
        "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "            ]\n",
        "\n",
        "            num_gts = len(ground_truth_img)\n",
        "            best_iou = 0\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = intersection_over_union(\n",
        "                    torch.tensor(detection[3:]),\n",
        "                    torch.tensor(gt[3:]),\n",
        "                    box_format=box_format,\n",
        "                )\n",
        "\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                # only detect ground truth detection once\n",
        "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                    # true positive and add this bounding box to seen\n",
        "                    TP[detection_idx] = 1\n",
        "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "                else:\n",
        "                    FP[detection_idx] = 1\n",
        "\n",
        "            # if IOU is lower then the detection is a false positive\n",
        "            else:\n",
        "                FP[detection_idx] = 1\n",
        "\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "        # torch.trapz for numerical integration\n",
        "        average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions)"
      ],
      "metadata": {
        "id": "ObJbSwQLgzH3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get bounding boxes and convert cells to bboxes"
      ],
      "metadata": {
        "id": "QyDN1kK3g2bH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_evaluation_bboxes(\n",
        "    loader,\n",
        "    model,\n",
        "    iou_threshold,\n",
        "    anchors,\n",
        "    threshold,\n",
        "    box_format=\"midpoint\",\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "):\n",
        "    # make sure model is in eval before get bboxes\n",
        "    model.eval()\n",
        "    train_idx = 0\n",
        "    all_pred_boxes = []\n",
        "    all_true_boxes = []\n",
        "    for batch_idx, (x, labels) in enumerate(loader):\n",
        "        x = x.float().to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        bboxes = [[] for _ in range(batch_size)]\n",
        "        for i in range(3):\n",
        "            S = predictions[i].shape[2] # grid cell size for each predictions\n",
        "            anchor = torch.tensor([*anchors[i]]).to(device) * S # anchor for each grid, prediction type\n",
        "            boxes_scale_i = cells_to_bboxes( # get bboxes for each image in the batch\n",
        "                predictions[i], anchor, S=S, is_preds=True\n",
        "            )\n",
        "            for idx, (box) in enumerate(boxes_scale_i): # for each image, append the bbox to corr. bboxes[idx]\n",
        "                bboxes[idx] += box\n",
        "\n",
        "        # we just want one bbox for each label, not one for each scale\n",
        "        true_bboxes = cells_to_bboxes(\n",
        "            labels[2], anchor, S=S, is_preds=False\n",
        "        )\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            nms_boxes = non_max_suppression(\n",
        "                bboxes[idx],\n",
        "                iou_threshold=iou_threshold,\n",
        "                threshold=threshold,\n",
        "                box_format=box_format,\n",
        "            )\n",
        "\n",
        "            for nms_box in nms_boxes:\n",
        "                all_pred_boxes.append([train_idx] + nms_box)\n",
        "\n",
        "            for box in true_bboxes[idx]:\n",
        "                if box[1] > threshold:\n",
        "                    all_true_boxes.append([train_idx] + box)\n",
        "\n",
        "            train_idx += 1\n",
        "\n",
        "    model.train()\n",
        "    return all_pred_boxes, all_true_boxes"
      ],
      "metadata": {
        "id": "cw3Zwv9ig6ZL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cells_to_bboxes(predictions, anchors, S, is_preds=True):\n",
        "    \"\"\"\n",
        "    Scales the predictions coming from the model to\n",
        "    be relative to the entire image such that they for example later\n",
        "    can be plotted or.\n",
        "    INPUT:\n",
        "    predictions: tensor of size (N, 3, S, S, num_classes+5)\n",
        "    anchors: the anchors used for the predictions\n",
        "    S: the number of cells the image is divided in on the width (and height)\n",
        "    is_preds: whether the input is predictions or the true bounding boxes\n",
        "    OUTPUT:\n",
        "    converted_bboxes: the converted boxes of sizes (N, num_anchors, S, S, 1+5) with class index,\n",
        "                      object score, bounding box coordinates\n",
        "    \"\"\"\n",
        "    BATCH_SIZE = predictions.shape[0]\n",
        "    num_anchors = len(anchors)\n",
        "    box_predictions = predictions[..., 1:5]\n",
        "    if is_preds:\n",
        "        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n",
        "        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
        "        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n",
        "        scores = torch.sigmoid(predictions[..., 0:1])\n",
        "        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n",
        "    else:\n",
        "        scores = predictions[..., 0:1]\n",
        "        best_class = predictions[..., 5:6]\n",
        "\n",
        "    cell_indices = (\n",
        "        torch.arange(S)\n",
        "        .repeat(predictions.shape[0], 3, S, 1)\n",
        "        .unsqueeze(-1)\n",
        "        .to(predictions.device)\n",
        "    )\n",
        "    x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\n",
        "    y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n",
        "    w_h = 1 / S * box_predictions[..., 2:4]\n",
        "    converted_bboxes = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n",
        "    return converted_bboxes.tolist()"
      ],
      "metadata": {
        "id": "06mXhrDJg9zv"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "--AS2dslvM-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = YOLOv3(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), lr=LEARNING_RATE\n",
        ")\n",
        "loss_fn = YoloLoss()\n",
        "\n",
        "# Scaler\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Train-Test Loader\n",
        "train_loader, test_loader = get_loaders(\n",
        "    train_csv_path='/content/drive/MyDrive/500examples.csv', test_csv_path='/content/drive/MyDrive/500examples.csv'\n",
        ")\n",
        "\n",
        "# Anchors\n",
        "scaled_anchors = (\n",
        "    torch.tensor(ANCHORS) * torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1,3,2)\n",
        ").to(DEVICE)"
      ],
      "metadata": {
        "id": "Si5AXfMMESev"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "for epoch in tqdm(range(NUM_EPOCHS), desc=\"Epochs\"):\n",
        "  model.train()\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  start_time = time.time() # Start time of the epoch\n",
        "\n",
        "  for batch_idx, (x,y) in enumerate(train_loader):\n",
        "    x = x.to(DEVICE)\n",
        "    y0, y1, y2 = (y[0].to(DEVICE),\n",
        "                  y[1].to(DEVICE),\n",
        "                  y[2].to(DEVICE))\n",
        "\n",
        "    # context manager is used in PyTorch to automatically handle mixed-precision computations on CUDA-enabled GPUs\n",
        "    with torch.cuda.amp.autocast():\n",
        "      out = model(x)\n",
        "      loss = (\n",
        "          loss_fn(out[0], y0, scaled_anchors[0])\n",
        "          + loss_fn(out[1], y1, scaled_anchors[1])\n",
        "          + loss_fn(out[2], y2, scaled_anchors[2])\n",
        "      )\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "  end_time = time.time()  # End time of the epoch\n",
        "  epoch_duration = end_time - start_time  # Duration of the epoch\n",
        "\n",
        "  if (epoch+1) % 2 == 0:\n",
        "    # Print the epoch duration\n",
        "    tqdm.write(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\")\n",
        "\n",
        "    # Print the loss and accuracy for training and validation data\n",
        "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], \"\n",
        "          f\"Loss: {sum(losses)/len(losses):.4f}\")\n",
        "\n",
        "    # save the model after every 10 epoch\n",
        "    torch.save(model.state_dict(), f'Yolov3_epoch{epoch+1}.pth')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxkC5a_lFOvA",
        "outputId": "e8f0926f-4861-4ad3-f85e-275a648b8085"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:   5%|▌         | 1/20 [04:11<1:14:12, 234.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 completed in 16.83 seconds\n",
            "Epoch [2/20], Loss: 55.4337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  15%|█▌        | 3/20 [04:45<18:35, 65.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 completed in 16.64 seconds\n",
            "Epoch [4/20], Loss: 51.2067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  25%|██▌       | 5/20 [05:19<08:58, 35.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 completed in 16.57 seconds\n",
            "Epoch [6/20], Loss: 46.5890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  35%|███▌      | 7/20 [05:53<05:28, 25.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 completed in 16.80 seconds\n",
            "Epoch [8/20], Loss: 43.9558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  45%|████▌     | 9/20 [06:27<03:50, 20.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 completed in 16.69 seconds\n",
            "Epoch [10/20], Loss: 41.4720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  55%|█████▌    | 11/20 [07:01<02:49, 18.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 completed in 16.86 seconds\n",
            "Epoch [12/20], Loss: 39.5184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  65%|██████▌   | 13/20 [07:35<02:05, 17.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 completed in 16.71 seconds\n",
            "Epoch [14/20], Loss: 37.5306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  75%|███████▌  | 15/20 [08:10<01:27, 17.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 completed in 17.21 seconds\n",
            "Epoch [16/20], Loss: 35.2183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  85%|████████▌ | 17/20 [08:44<00:52, 17.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 completed in 16.70 seconds\n",
            "Epoch [18/20], Loss: 33.9645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:  95%|█████████▌| 19/20 [09:19<00:17, 17.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 completed in 17.06 seconds\n",
            "Epoch [20/20], Loss: 32.2851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs: 100%|██████████| 20/20 [09:19<00:00, 27.98s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_class_accuracy(model, train_loader, threshold=CONF_THRESHOLD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5CMaxO6LDvH",
        "outputId": "5ea508c9-c38f-4057-bb43-900c030b3fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:02<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class accuracy is: 87.698410%\n",
            "No obj accuracy is: 0.030934%\n",
            "Obj accuracy is: 100.000000%\n"
          ]
        }
      ]
    }
  ]
}